{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NpMpA6iTxi6h"
   },
   "outputs": [],
   "source": [
    "!nbstripout \"/content/drive/MyDrive/msu-nlp/XLM-R.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Xme2pdmCzNc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXZ5hiKqDr8o"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/msu-nlp/train_translated_full.csv\")\n",
    "\n",
    "df[[\"question\", \"answer_text\"]].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-NChoIsD6SS"
   },
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mL9IcfVlD88w"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5EcSw_0EHrc"
   },
   "outputs": [],
   "source": [
    "def translate_to_hi(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "\n",
    "    # Encode\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Hindi language token\n",
    "    hi_token_id = tokenizer.convert_tokens_to_ids(\"hi_Deva\")\n",
    "\n",
    "    # Generate translation\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=hi_token_id\n",
    "        )\n",
    "\n",
    "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWXXXQ7jHBup"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/content/drive/MyDrive/msu-nlp/train_translated_full.csv\")\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCqHqne5Mi8K"
   },
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece accelerate -q\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load datasets\n",
    "train = pd.read_csv(\"/content/drive/MyDrive/msu-nlp/train_translated_full.csv\")\n",
    "dev   = pd.read_csv(\"/content/drive/MyDrive/msu-nlp/dev_translated_full.csv\")\n",
    "\n",
    "# Load EN ‚Üí HI translation model\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# üöÄ STEP 1 ‚Äî Extract UNIQUE answers only\n",
    "# -----------------------------------------\n",
    "all_answers = pd.concat([train[\"answer_text\"], dev[\"answer_text\"]], ignore_index=True)\n",
    "unique_answers = list(all_answers.dropna().unique())\n",
    "\n",
    "print(\"Unique answers:\", len(unique_answers))\n",
    "\n",
    "# -----------------------------------------\n",
    "# üöÄ STEP 2 ‚Äî Batch translation on GPU\n",
    "# -----------------------------------------\n",
    "def batch_translate(texts, batch_size=64):\n",
    "    outputs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Batch Translating\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tokenized = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**tokenized, max_length=40)\n",
    "        decoded = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        outputs.extend(decoded)\n",
    "    return outputs\n",
    "\n",
    "# Perform fast batch translation\n",
    "translated_list = batch_translate(unique_answers, batch_size=64)\n",
    "\n",
    "# Make a mapping: English ‚Üí Hindi\n",
    "translated_dict = {eng: hin for eng, hin in zip(unique_answers, translated_list)}\n",
    "\n",
    "# -----------------------------------------\n",
    "# üöÄ STEP 3 ‚Äî Assign translations back\n",
    "# -----------------------------------------\n",
    "train[\"answer_text_hi\"] = train[\"answer_text\"].map(translated_dict)\n",
    "dev[\"answer_text_hi\"]   = dev[\"answer_text\"].map(translated_dict)\n",
    "\n",
    "# -----------------------------------------\n",
    "# üöÄ STEP 4 ‚Äî Save translated datasets\n",
    "# -----------------------------------------\n",
    "train_out = \"/content/drive/MyDrive/msu-nlp/train_translated_full_with_hi_answers.csv\"\n",
    "dev_out   = \"/content/drive/MyDrive/msu-nlp/dev_translated_full_with_hi_answers.csv\"\n",
    "\n",
    "train.to_csv(train_out, index=False)\n",
    "dev.to_csv(dev_out, index=False)\n",
    "\n",
    "print(\"üî• DONE ‚Äî Batch translation finished FAST!\")\n",
    "print(train_out)\n",
    "print(dev_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_JKklX_TzFo"
   },
   "outputs": [],
   "source": [
    "train[[\"answer_text\", \"answer_text_hi\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2NMPYCTT3aK"
   },
   "outputs": [],
   "source": [
    "dev[[\"answer_text\", \"answer_text_hi\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtERX90ZUCzG"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate sentencepiece accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhO-eHaDUNI1"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_path = \"/content/drive/MyDrive/msu-nlp/train_translated_full_with_hi_answers.csv\"\n",
    "dev_path   = \"/content/drive/MyDrive/msu-nlp/dev_translated_full_with_hi_answers.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "dev   = pd.read_csv(dev_path)\n",
    "\n",
    "train.head()\n",
    "dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAj2-1cUUVOH"
   },
   "outputs": [],
   "source": [
    "!pip install fugashi python-Levenshtein rapidfuzz -q\n",
    "from rapidfuzz import fuzz, process\n",
    "import re\n",
    "import unicodedata\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# NORMALIZATION UTILITIES\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def normalize_text(txt):\n",
    "    if not isinstance(txt, str):\n",
    "        return \"\"\n",
    "    txt = unicodedata.normalize(\"NFKC\", txt)\n",
    "    txt = txt.replace(\"‡•§\", \" \")\n",
    "    txt = re.sub(r\"[^\\w\\s\\u0900-\\u097F]\", \" \", txt)   # keep Devanagari + whitespace\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "\n",
    "def strip_postpositions(txt):\n",
    "    \"\"\"Remove common Hindi suffixes that appear after locations/numbers.\"\"\"\n",
    "    for p in [\"‡§Æ‡•á‡§Ç\", \"‡§ï‡§æ\", \"‡§ï‡•Ä\", \"‡§ï‡•á\", \"‡§∏‡•á\", \"‡§ï‡•ã\", \"‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ\"]:\n",
    "        if txt.endswith(\" \" + p):\n",
    "            return txt[: -len(p) - 1]\n",
    "    return txt\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# SMART MATCHER (3-PASS STRATEGY)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def find_best_answer_start(context, answer):\n",
    "    \"\"\"\n",
    "    Returns best answer_start index using:\n",
    "    1) Exact match\n",
    "    2) Normalized match\n",
    "    3) Fuzzy token-level matching\n",
    "    \"\"\"\n",
    "\n",
    "    # PASS 1 ‚Äî Exact match\n",
    "    exact = context.find(answer)\n",
    "    if exact != -1:\n",
    "        return exact\n",
    "\n",
    "    # Prepare normalized versions\n",
    "    context_norm = normalize_text(context)\n",
    "    answer_norm  = normalize_text(answer)\n",
    "    answer_norm  = strip_postpositions(answer_norm)\n",
    "\n",
    "    # PASS 2 ‚Äî Normalized exact match\n",
    "    exact_norm = context_norm.find(answer_norm)\n",
    "    if exact_norm != -1:\n",
    "        return exact_norm\n",
    "\n",
    "    # PASS 3 ‚Äî Fuzzy matching (per-token)\n",
    "    tokens = answer_norm.split()\n",
    "    best_score = -1\n",
    "    best_index = -1\n",
    "\n",
    "    for token in tokens:\n",
    "        if len(token) < 2:\n",
    "            continue\n",
    "\n",
    "        # Search token in context_tokens via fuzzy matching\n",
    "        matches = process.extract(token, context_norm.split(), scorer=fuzz.partial_ratio, limit=3)\n",
    "\n",
    "        for match_text, score, match_index in matches:\n",
    "            if score > best_score and score > 70:  # threshold\n",
    "                # reconstruct approx character index\n",
    "                approx_start = context_norm.find(match_text)\n",
    "                if approx_start != -1:\n",
    "                    best_score = score\n",
    "                    best_index = approx_start\n",
    "\n",
    "    return best_index\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# NEW BUILD FUNCTION USING FUZZY MATCHING\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def build_squad_json_fuzzy(df, out_path):\n",
    "    df = df.fillna(\"\")\n",
    "\n",
    "    out = {\"data\": []}\n",
    "    paragraphs = {}\n",
    "\n",
    "    skipped_missing = 0\n",
    "    unmatched = 0\n",
    "    matched = 0\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Building fuzzy SQuAD\"):\n",
    "\n",
    "        title = row[\"title\"]\n",
    "        context = row[\"context_hi\"]\n",
    "        question = row[\"question_hi\"]\n",
    "        answer = row[\"answer_text_hi\"]\n",
    "\n",
    "        if not isinstance(context, str) or not isinstance(question, str) or not isinstance(answer, str):\n",
    "            skipped_missing += 1\n",
    "            continue\n",
    "\n",
    "        if context.strip() == \"\" or question.strip() == \"\" or answer.strip() == \"\":\n",
    "            skipped_missing += 1\n",
    "            continue\n",
    "\n",
    "        # Create entry for this title/context\n",
    "        if title not in paragraphs:\n",
    "            paragraphs[title] = {}\n",
    "        if context not in paragraphs[title]:\n",
    "            paragraphs[title][context] = {\n",
    "                \"context\": context,\n",
    "                \"qas\": []\n",
    "            }\n",
    "\n",
    "        # Fuzzy matching for answer start\n",
    "        answer_start = find_best_answer_start(context, answer)\n",
    "\n",
    "        if answer_start == -1:\n",
    "            unmatched += 1\n",
    "            continue\n",
    "\n",
    "        matched += 1\n",
    "\n",
    "        qas_item = {\n",
    "            \"id\": f\"{title}-{idx}\",\n",
    "            \"question\": question,\n",
    "            \"answers\": [{\n",
    "                \"text\": answer,\n",
    "                \"answer_start\": answer_start\n",
    "            }]\n",
    "        }\n",
    "\n",
    "        paragraphs[title][context][\"qas\"].append(qas_item)\n",
    "\n",
    "    # Convert to SQuAD format\n",
    "    for title, contexts in paragraphs.items():\n",
    "        out[\"data\"].append({\n",
    "            \"title\": title,\n",
    "            \"paragraphs\": list(contexts.values())\n",
    "        })\n",
    "\n",
    "    # Save\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"\\nSaved:\", out_path)\n",
    "    print(\"Matched:\", matched)\n",
    "    print(\"Unmatched:\", unmatched)\n",
    "    print(\"Missing:\", skipped_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJYq3GeAUmJ9"
   },
   "outputs": [],
   "source": [
    "build_squad_json_fuzzy(train, \"/content/train_hi_squad_fuzzy.json\")\n",
    "build_squad_json_fuzzy(dev,   \"/content/dev_hi_squad_fuzzy.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jyFzyx9UtfC"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate sentencepiece accelerate -q\n",
    "\n",
    "import json, collections\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sI7Hu0jrWOKj"
   },
   "outputs": [],
   "source": [
    "train_json_path = \"/content/train_hi_squad_fuzzy.json\"\n",
    "dev_json_path   = \"/content/dev_hi_squad_fuzzy.json\"\n",
    "\n",
    "def load_and_flatten_squad(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        squad = json.load(f)\n",
    "\n",
    "    rows = []\n",
    "    for article in squad[\"data\"]:\n",
    "        title = article.get(\"title\", \"\")\n",
    "        for para in article[\"paragraphs\"]:\n",
    "            context = para[\"context\"]\n",
    "            for qa in para[\"qas\"]:\n",
    "                rows.append({\n",
    "                    \"id\": qa[\"id\"],\n",
    "                    \"title\": title,\n",
    "                    \"context\": context,\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"answers\": {\n",
    "                        \"text\": [qa[\"answers\"][0][\"text\"]],\n",
    "                        \"answer_start\": [qa[\"answers\"][0][\"answer_start\"]]\n",
    "                    }\n",
    "                })\n",
    "    return Dataset.from_list(rows)\n",
    "\n",
    "train_ds = load_and_flatten_squad(train_json_path)\n",
    "dev_ds   = load_and_flatten_squad(dev_json_path)\n",
    "\n",
    "print(train_ds)\n",
    "print(dev_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CIR2IcLWT7R"
   },
   "outputs": [],
   "source": [
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "def clean_examples(examples):\n",
    "    valid_mask = [\n",
    "        (q is not None and isinstance(q, str) and q.strip() != \"\") and\n",
    "        (c is not None and isinstance(c, str) and c.strip() != \"\")\n",
    "        for q, c in zip(examples[\"question\"], examples[\"context\"])\n",
    "    ]\n",
    "    for key in examples.keys():\n",
    "        examples[key] = [v for v, ok in zip(examples[key], valid_mask) if ok]\n",
    "    return examples\n",
    "\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    examples = clean_examples(examples)\n",
    "\n",
    "    questions = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_token_type_ids=False,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "\n",
    "        sample_idx = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_idx]\n",
    "        start_char = answers[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "        # Find start of context\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "\n",
    "        # End of context\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "\n",
    "        if not (\n",
    "            offsets[token_start_index][0] <= start_char and\n",
    "            offsets[token_end_index][1] >= end_char\n",
    "        ):\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            start_positions.append(token_start_index - 1)\n",
    "\n",
    "            while offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            end_positions.append(token_end_index + 1)\n",
    "\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def prepare_validation_features(examples):\n",
    "    examples = clean_examples(examples)\n",
    "\n",
    "    questions = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_token_type_ids=False,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    tokenized[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized[\"input_ids\"])):\n",
    "        sample_idx = sample_mapping[i]\n",
    "        tokenized[\"example_id\"].append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        offsets = tokenized[\"offset_mapping\"][i]\n",
    "\n",
    "        tokenized[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None\n",
    "            for k, o in enumerate(offsets)\n",
    "        ]\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "train_feats = train_ds.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=train_ds.column_names\n",
    ")\n",
    "\n",
    "dev_feats = dev_ds.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=dev_ds.column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkV1UM4EXZNv"
   },
   "outputs": [],
   "source": [
    "#Post-process predictions + compute EM/F1\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size=20, max_answer_length=30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, f in enumerate(features):\n",
    "        features_per_example[example_id_to_index[f[\"example_id\"]]].append(i)\n",
    "\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    for example_index, example in enumerate(examples):\n",
    "        feature_indices = features_per_example[example_index]\n",
    "        context = example[\"context\"]\n",
    "\n",
    "        valid_answers = []\n",
    "\n",
    "        for fi in feature_indices:\n",
    "            start_logits = all_start_logits[fi]\n",
    "            end_logits = all_end_logits[fi]\n",
    "            offsets = features[fi][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logits)[-n_best_size:][::-1]\n",
    "            end_indexes = np.argsort(end_logits)[-n_best_size:][::-1]\n",
    "\n",
    "            for s in start_indexes:\n",
    "                for e in end_indexes:\n",
    "                    if offsets[s] is None or offsets[e] is None:\n",
    "                        continue\n",
    "                    if e < s or (e - s + 1) > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offsets[s][0]\n",
    "                    end_char = offsets[e][1]\n",
    "                    valid_answers.append({\n",
    "                        \"score\": start_logits[s] + end_logits[e],\n",
    "                        \"text\": context[start_char:end_char]\n",
    "                    })\n",
    "\n",
    "        if valid_answers:\n",
    "            best = max(valid_answers, key=lambda x: x[\"score\"])\n",
    "            predictions[example[\"id\"]] = best[\"text\"]\n",
    "        else:\n",
    "            predictions[example[\"id\"]] = \"\"\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = postprocess_qa_predictions(dev_ds, dev_feats, p.predictions)\n",
    "    formatted_preds = [{\"id\": k, \"prediction_text\": v} for k, v in preds.items()]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in dev_ds]\n",
    "    return squad_metric.compute(predictions=formatted_preds, references=references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_yfR0ecYc8x"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Train XLM-R\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/xlmr_hi_fuzzy\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_steps=200,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_feats,\n",
    "    eval_dataset=dev_feats,\n",
    "    tokenizer=tokenizer,  # the future warning is harmless\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YBp64yEXog2"
   },
   "outputs": [],
   "source": [
    "#Final Eval\n",
    "final_results = trainer.evaluate()\n",
    "print(final_results)\n",
    "\n",
    "save_path = \"/content/drive/MyDrive/msu-nlp/xlmr_hi_fuzzy_trained\"\n",
    "trainer.save_model(save_path)\n",
    "print(\"Saved model to:\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkWWMaQAsAvb"
   },
   "outputs": [],
   "source": [
    "print(\"üìå FINAL RESULTS (Hindi XLM-R QA)\")\n",
    "for k, v in final_results.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4rdyh8EsK-G"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# 1) Get raw logits from model on dev set\n",
    "raw_predictions = trainer.predict(dev_feats)\n",
    "\n",
    "# 2) Use our postprocessing function to turn logits ‚Üí text predictions\n",
    "predictions = postprocess_qa_predictions(\n",
    "    examples=dev_ds,\n",
    "    features=dev_feats,\n",
    "    raw_predictions=raw_predictions.predictions\n",
    ")\n",
    "\n",
    "# 3) Format predictions + references\n",
    "formatted_predictions = [\n",
    "    {\"id\": k, \"prediction_text\": v}\n",
    "    for k, v in predictions.items()\n",
    "]\n",
    "\n",
    "references = [\n",
    "    {\"id\": example[\"id\"], \"answers\": example[\"answers\"]}\n",
    "    for example in dev_ds\n",
    "]\n",
    "\n",
    "# 4) Compute SQuAD metrics\n",
    "results = squad_metric.compute(\n",
    "    predictions=formatted_predictions,\n",
    "    references=references\n",
    ")\n",
    "\n",
    "print(\"üìå FINAL RESULTS (Hindi XLM-R QA)\")\n",
    "print(\"Exact Match (EM):\", results[\"exact_match\"])\n",
    "print(\"F1 Score:\", results[\"f1\"])\n",
    "print(\"Validation Samples:\", len(dev_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvlJL5HbuwkE"
   },
   "outputs": [],
   "source": [
    "#Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFXkcJSfvRhE"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load your LaBSE output CSVs\n",
    "train_out = pd.read_csv(\"/content/drive/MyDrive/msu-nlp/train_with_labse_scores.csv\")\n",
    "dev_out   = pd.read_csv(\"/content/drive/MyDrive/msu-nlp/dev_with_labse_scores.csv\")\n",
    "\n",
    "# Now extract similarity columns\n",
    "train_context_sim = train_out[\"sim_context\"]\n",
    "train_question_sim = train_out[\"sim_question\"]\n",
    "\n",
    "dev_context_sim = dev_out[\"sim_context\"]\n",
    "dev_question_sim = dev_out[\"sim_question\"]\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# SAMPLE VARIABLES (insert your real results)\n",
    "# ----------------------------------\n",
    "train_context_sim = train_out[\"sim_context\"]\n",
    "train_question_sim = train_out[\"sim_question\"]\n",
    "\n",
    "dev_context_sim = dev_out[\"sim_context\"]\n",
    "dev_question_sim = dev_out[\"sim_question\"]\n",
    "\n",
    "em_score = 12.30\n",
    "f1_score = 26.55\n",
    "\n",
    "# --------------------------------------------\n",
    "# 1) BAR CHART ‚Äî Hindi XLM-R QA Performance\n",
    "# --------------------------------------------\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar([\"Exact Match\", \"F1 Score\"], [em_score, f1_score], color=[\"#4C72B0\",\"#55A868\"])\n",
    "plt.title(\"Hindi QA Performance (XLM-R Base)\")\n",
    "plt.ylabel(\"Score (%)\")\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) KDE DISTRIBUTION ‚Äî LaBSE Semantic Similarity (Train/Dev)\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.kdeplot(train_context_sim, label=\"Train Context\", linewidth=2)\n",
    "sns.kdeplot(train_question_sim, label=\"Train Question\", linewidth=2)\n",
    "sns.kdeplot(dev_context_sim, label=\"Dev Context\", linewidth=2)\n",
    "sns.kdeplot(dev_question_sim, label=\"Dev Question\", linewidth=2)\n",
    "plt.title(\"LaBSE Semantic Similarity Distribution\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.grid(linestyle=\"--\", alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 3) BOXPLOTS ‚Äî Train vs Dev with IQR Highlight\n",
    "# --------------------------------------------\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(data=[\n",
    "    train_context_sim,\n",
    "    train_question_sim,\n",
    "    dev_context_sim,\n",
    "    dev_question_sim\n",
    "])\n",
    "plt.xticks(\n",
    "    [0,1,2,3],\n",
    "    [\"Train-Context\", \"Train-Question\", \"Dev-Context\", \"Dev-Question\"],\n",
    "    rotation=20\n",
    ")\n",
    "plt.title(\"LaBSE Similarity ‚Äî IQR Overview\")\n",
    "plt.ylabel(\"Cosine Similarity\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTi2h1ixxDm3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "em = 12.30\n",
    "f1 = 26.55\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar([\"Exact Match\", \"F1 Score\"], [em, f1],\n",
    "        color=[\"#6A5ACD\",\"#20B2AA\"], edgecolor='black')\n",
    "plt.title(\"XLM-R Hindi QA Performance\", fontsize=14)\n",
    "plt.ylabel(\"Score (%)\")\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(linestyle=\"--\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ic2RoZRuxJGm"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Averages from your LaBSE results\n",
    "avg_context_sim = train_context_sim.mean()\n",
    "avg_question_sim = train_question_sim.mean()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar([\"Context Sim\", \"Question Sim\", \"EM\", \"F1\"],\n",
    "        [avg_context_sim*100, avg_question_sim*100, em, f1],\n",
    "        color=[\"#1f77b4\",\"#aec7e8\",\"#ff7f0e\",\"#ffbb78\"],\n",
    "        edgecolor='black')\n",
    "plt.title(\"LaBSE vs XLM-R Performance Comparison\", fontsize=14)\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}